{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/titli/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing function from a different ipny \n",
    "import pandas as pd\n",
    "import collections\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import operator\n",
    "import sys\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from functools import reduce\n",
    "import spacy\n",
    "from keras import backend as K\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "pd.options.display.max_columns = 1000\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Input, BatchNormalization, MaxPooling1D, Bidirectional,LSTM\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPool1D, Flatten , Embedding, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import adam\n",
    "#https://machinelearningmastery.com/deep-learning-bag-of-words-model-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NUM_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>isspam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Confidential :Soma:, Ci@lis, :P:ntermin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¢Â ÇªÁüÇÑ ¼³³¯ ÀÌº¥Æ® ÀÀ¸ðÇØ¼­ ºÎ¸ð´Ô²² ¼±¹°ÇÏ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_na_</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>StOck 0ppurtunities - their sh0Oting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>All your prescr[iption needs right here</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Subject  isspam\n",
       "0            Confidential :Soma:, Ci@lis, :P:ntermin       1\n",
       "1  ¢Â ÇªÁüÇÑ ¼³³¯ ÀÌº¥Æ® ÀÀ¸ðÇØ¼­ ºÎ¸ð´Ô²² ¼±¹°ÇÏ...       1\n",
       "2                                               _na_       1\n",
       "3               StOck 0ppurtunities - their sh0Oting       1\n",
       "6            All your prescr[iption needs right here       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_data = pd.read_csv('subject_spam.csv', index_col=0, encoding='utf8', engine='python')\n",
    "keras_data.fillna(\"_na_\", inplace = True)\n",
    "keras_data = keras_data.drop(keras_data[keras_data.isspam == \"_na_\"].index)\n",
    "spammer = {'spam ': 1,'ham ': 0} \n",
    "keras_data.isspam = [spammer[item] for item in keras_data.isspam] \n",
    "spam_index = keras_data[keras_data.isspam ==1].index\n",
    "ham_index = keras_data[keras_data.isspam == 0].index\n",
    "new_index = np.concatenate((spam_index[:10000], ham_index[:9997]), axis=0)\n",
    "keras_data_new = keras_data.iloc[new_index]\n",
    "labels = keras_data_new.isspam\n",
    "keras_data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join('glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15425 unique tokens.\n",
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(keras_data_new.Subject)\n",
    "sequences = tokenizer.texts_to_sequences(keras_data_new.Subject)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print('Training model.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(fl1=16, fl2=16, fl3=16, dl=16, optimizer= 'RMSprop', kl = 5, layer =1 ):\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    if (layer == 1):\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "    elif (layer == 2):\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl2, kernel_size = kl, activation='relu')(x)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "    else:\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl2, kernel_size = kl, activation='relu')(x)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl3, kernel_size = kl, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(units = dl, activation='relu')(x)\n",
    "    preds = Dense(1, activation='tanh')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "              optimizer= optimizer,\n",
    "              metrics=['acc'])\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_edit(out):\n",
    "    loss_list = [s for s in out.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in out.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in out.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in out.history.keys() if 'acc' in s and 'val' in s]\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    for l in loss_list:\n",
    "        train_loss .append(out.history[l])\n",
    "    for l in val_loss_list:\n",
    "        val_loss .append(out.history[l])\n",
    "    for l in acc_list:\n",
    "        train_acc.append(out.history[l])\n",
    "    for l in val_acc_list:\n",
    "        val_acc .append(out.history[l])\n",
    "    line  = { 'train_loss': reduce(operator.concat, train_loss)[-1],'val_loss': reduce(operator.concat, val_loss)[-1], \\\n",
    "             'train_acc': reduce(operator.concat, train_acc)[-1], 'val_acc': reduce(operator.concat, val_acc)[-1] }\n",
    "    return (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kera_trainable_params(model):\n",
    "    trainable_count = int(np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
    "    non_trainable_count = int(np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])) \n",
    "    return trainable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 161s 10ms/step - loss: 0.4864 - acc: 0.7754 - val_loss: 0.3880 - val_acc: 0.8237\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 153s 10ms/step - loss: 0.3703 - acc: 0.8138 - val_loss: 0.4661 - val_acc: 0.7594\n",
      "{'train_loss': 0.37029239110781825, 'val_loss': 0.46606014205026874, 'train_acc': 0.8137892236604082, 'val_acc': 0.759439859994801}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "   80/15998 [..............................] - ETA: 6:58 - loss: 8.4620 - acc: 0.4375"
     ]
    }
   ],
   "source": [
    "layers = [1,2,3]\n",
    "fl1s = [16,32,64, 128]\n",
    "fl2s = [16,32,64, 128]\n",
    "fl3s = [16,32,64, 128]\n",
    "kls = [5] \n",
    "dls = [16,32,64, 128]\n",
    "\n",
    "trainable_count = []\n",
    "optimizers = ['Adam', 'Nadam', 'RMSprop','sgd'] \n",
    "loss_data = []\n",
    "params = []\n",
    "for fl1, fl2, fl3, kl, dl, optimizer,layer in product(fl1s,fl2s,fl3s,kls,dls,optimizers,layers):\n",
    "    kwargs = dict(fl1=fl1, fl2= fl2, fl3=fl3, kl=kl, dl=dl, optimizer= ''.join(optimizer), layer=layer)\n",
    "    params.append(kwargs)\n",
    "    model = embeddings(**kwargs)\n",
    "    trainable_count.append(kera_trainable_params(model))\n",
    "    history = model.fit(x_train, y_train, batch_size= 16, epochs=2, validation_data=(x_val, y_val))\n",
    "    loss_data.append(loss_edit(history))\n",
    "    print(loss_edit(history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here an example model is shown. This type of analysis is done for many more models not shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_pd = pd.DataFrame(params)\n",
    "loss_pd= pd.DataFrame(loss_data)\n",
    "trainable_pd = pd.DataFrame({'trainable_params': trainable_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.concat([params_pd, loss_pd, trainable_pd], axis=1, join='inner')\n",
    "embeddings.to_csv('embeddings_models.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dl</th>\n",
       "      <th>fl1</th>\n",
       "      <th>fl2</th>\n",
       "      <th>fl3</th>\n",
       "      <th>kl</th>\n",
       "      <th>layer</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>trainable_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.921803</td>\n",
       "      <td>0.181132</td>\n",
       "      <td>0.901725</td>\n",
       "      <td>0.230709</td>\n",
       "      <td>66226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.917302</td>\n",
       "      <td>0.191708</td>\n",
       "      <td>0.902476</td>\n",
       "      <td>0.231941</td>\n",
       "      <td>75602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.918865</td>\n",
       "      <td>0.190353</td>\n",
       "      <td>0.900975</td>\n",
       "      <td>0.233088</td>\n",
       "      <td>76290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.915677</td>\n",
       "      <td>0.195752</td>\n",
       "      <td>0.895224</td>\n",
       "      <td>0.233315</td>\n",
       "      <td>56898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.918740</td>\n",
       "      <td>0.188945</td>\n",
       "      <td>0.899725</td>\n",
       "      <td>0.234236</td>\n",
       "      <td>76818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.914739</td>\n",
       "      <td>0.192709</td>\n",
       "      <td>0.896474</td>\n",
       "      <td>0.234565</td>\n",
       "      <td>80898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.181003</td>\n",
       "      <td>0.899725</td>\n",
       "      <td>0.235206</td>\n",
       "      <td>80898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.919240</td>\n",
       "      <td>0.189732</td>\n",
       "      <td>0.899725</td>\n",
       "      <td>0.235302</td>\n",
       "      <td>33138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.922240</td>\n",
       "      <td>0.178402</td>\n",
       "      <td>0.901475</td>\n",
       "      <td>0.235378</td>\n",
       "      <td>66226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.914927</td>\n",
       "      <td>0.196061</td>\n",
       "      <td>0.898225</td>\n",
       "      <td>0.235445</td>\n",
       "      <td>68322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.916302</td>\n",
       "      <td>0.193030</td>\n",
       "      <td>0.894974</td>\n",
       "      <td>0.236199</td>\n",
       "      <td>33138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.914614</td>\n",
       "      <td>0.200537</td>\n",
       "      <td>0.893473</td>\n",
       "      <td>0.236664</td>\n",
       "      <td>122754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.918927</td>\n",
       "      <td>0.192746</td>\n",
       "      <td>0.896224</td>\n",
       "      <td>0.236727</td>\n",
       "      <td>79218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.916177</td>\n",
       "      <td>0.197497</td>\n",
       "      <td>0.897224</td>\n",
       "      <td>0.237220</td>\n",
       "      <td>86850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.914739</td>\n",
       "      <td>0.201833</td>\n",
       "      <td>0.897724</td>\n",
       "      <td>0.237753</td>\n",
       "      <td>163586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.916865</td>\n",
       "      <td>0.194126</td>\n",
       "      <td>0.897974</td>\n",
       "      <td>0.238680</td>\n",
       "      <td>81538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.919240</td>\n",
       "      <td>0.185281</td>\n",
       "      <td>0.901725</td>\n",
       "      <td>0.238722</td>\n",
       "      <td>85762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.917740</td>\n",
       "      <td>0.192390</td>\n",
       "      <td>0.901475</td>\n",
       "      <td>0.238763</td>\n",
       "      <td>46818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.916865</td>\n",
       "      <td>0.194695</td>\n",
       "      <td>0.898475</td>\n",
       "      <td>0.238928</td>\n",
       "      <td>93138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.913864</td>\n",
       "      <td>0.199296</td>\n",
       "      <td>0.899475</td>\n",
       "      <td>0.238976</td>\n",
       "      <td>65042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.915114</td>\n",
       "      <td>0.199119</td>\n",
       "      <td>0.897474</td>\n",
       "      <td>0.239169</td>\n",
       "      <td>95906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.918115</td>\n",
       "      <td>0.187847</td>\n",
       "      <td>0.898725</td>\n",
       "      <td>0.239315</td>\n",
       "      <td>85202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.915239</td>\n",
       "      <td>0.195792</td>\n",
       "      <td>0.897474</td>\n",
       "      <td>0.239663</td>\n",
       "      <td>53682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.918365</td>\n",
       "      <td>0.187659</td>\n",
       "      <td>0.899975</td>\n",
       "      <td>0.240112</td>\n",
       "      <td>33138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.918677</td>\n",
       "      <td>0.189671</td>\n",
       "      <td>0.899475</td>\n",
       "      <td>0.240734</td>\n",
       "      <td>33138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.918802</td>\n",
       "      <td>0.188087</td>\n",
       "      <td>0.900725</td>\n",
       "      <td>0.241033</td>\n",
       "      <td>36354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.914302</td>\n",
       "      <td>0.202921</td>\n",
       "      <td>0.893473</td>\n",
       "      <td>0.241495</td>\n",
       "      <td>45218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.917427</td>\n",
       "      <td>0.191698</td>\n",
       "      <td>0.898225</td>\n",
       "      <td>0.241539</td>\n",
       "      <td>75986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.918177</td>\n",
       "      <td>0.188066</td>\n",
       "      <td>0.897474</td>\n",
       "      <td>0.241607</td>\n",
       "      <td>36354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.917115</td>\n",
       "      <td>0.188769</td>\n",
       "      <td>0.895974</td>\n",
       "      <td>0.241705</td>\n",
       "      <td>40642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.917615</td>\n",
       "      <td>0.187601</td>\n",
       "      <td>0.889972</td>\n",
       "      <td>0.281464</td>\n",
       "      <td>40642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.914489</td>\n",
       "      <td>0.199242</td>\n",
       "      <td>0.883471</td>\n",
       "      <td>0.281938</td>\n",
       "      <td>34210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.916615</td>\n",
       "      <td>0.191289</td>\n",
       "      <td>0.886972</td>\n",
       "      <td>0.282142</td>\n",
       "      <td>53682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.916302</td>\n",
       "      <td>0.194880</td>\n",
       "      <td>0.886722</td>\n",
       "      <td>0.282329</td>\n",
       "      <td>74690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.918427</td>\n",
       "      <td>0.191462</td>\n",
       "      <td>0.887722</td>\n",
       "      <td>0.282447</td>\n",
       "      <td>36354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.915364</td>\n",
       "      <td>0.194344</td>\n",
       "      <td>0.883471</td>\n",
       "      <td>0.283351</td>\n",
       "      <td>94786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.915927</td>\n",
       "      <td>0.191898</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.284476</td>\n",
       "      <td>88946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.914114</td>\n",
       "      <td>0.202205</td>\n",
       "      <td>0.891973</td>\n",
       "      <td>0.286213</td>\n",
       "      <td>56930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.915489</td>\n",
       "      <td>0.196842</td>\n",
       "      <td>0.871718</td>\n",
       "      <td>0.287318</td>\n",
       "      <td>53682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.919052</td>\n",
       "      <td>0.189296</td>\n",
       "      <td>0.883971</td>\n",
       "      <td>0.288285</td>\n",
       "      <td>77538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.914864</td>\n",
       "      <td>0.196835</td>\n",
       "      <td>0.890973</td>\n",
       "      <td>0.289141</td>\n",
       "      <td>33138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.914239</td>\n",
       "      <td>0.198099</td>\n",
       "      <td>0.881220</td>\n",
       "      <td>0.290793</td>\n",
       "      <td>60178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.911989</td>\n",
       "      <td>0.204128</td>\n",
       "      <td>0.883971</td>\n",
       "      <td>0.291366</td>\n",
       "      <td>40642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.916302</td>\n",
       "      <td>0.189844</td>\n",
       "      <td>0.891973</td>\n",
       "      <td>0.292030</td>\n",
       "      <td>75986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.921553</td>\n",
       "      <td>0.177740</td>\n",
       "      <td>0.883221</td>\n",
       "      <td>0.292098</td>\n",
       "      <td>72514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.917552</td>\n",
       "      <td>0.193023</td>\n",
       "      <td>0.880220</td>\n",
       "      <td>0.294988</td>\n",
       "      <td>44578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.915114</td>\n",
       "      <td>0.198183</td>\n",
       "      <td>0.877969</td>\n",
       "      <td>0.295547</td>\n",
       "      <td>122754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.915614</td>\n",
       "      <td>0.196222</td>\n",
       "      <td>0.877969</td>\n",
       "      <td>0.295568</td>\n",
       "      <td>33138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.916302</td>\n",
       "      <td>0.191133</td>\n",
       "      <td>0.880970</td>\n",
       "      <td>0.295794</td>\n",
       "      <td>34210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.915302</td>\n",
       "      <td>0.193594</td>\n",
       "      <td>0.876719</td>\n",
       "      <td>0.298285</td>\n",
       "      <td>34210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.916177</td>\n",
       "      <td>0.195439</td>\n",
       "      <td>0.884221</td>\n",
       "      <td>0.298465</td>\n",
       "      <td>38802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.910864</td>\n",
       "      <td>0.204749</td>\n",
       "      <td>0.877219</td>\n",
       "      <td>0.299356</td>\n",
       "      <td>40642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.915114</td>\n",
       "      <td>0.195504</td>\n",
       "      <td>0.883221</td>\n",
       "      <td>0.299551</td>\n",
       "      <td>77346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.919677</td>\n",
       "      <td>0.187814</td>\n",
       "      <td>0.877719</td>\n",
       "      <td>0.300084</td>\n",
       "      <td>34210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.919740</td>\n",
       "      <td>0.187301</td>\n",
       "      <td>0.876969</td>\n",
       "      <td>0.301659</td>\n",
       "      <td>76818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.918927</td>\n",
       "      <td>0.187650</td>\n",
       "      <td>0.883971</td>\n",
       "      <td>0.302694</td>\n",
       "      <td>38418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.913864</td>\n",
       "      <td>0.200447</td>\n",
       "      <td>0.876469</td>\n",
       "      <td>0.306944</td>\n",
       "      <td>47346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.920615</td>\n",
       "      <td>0.180782</td>\n",
       "      <td>0.877969</td>\n",
       "      <td>0.310799</td>\n",
       "      <td>74994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.916365</td>\n",
       "      <td>0.194129</td>\n",
       "      <td>0.877469</td>\n",
       "      <td>0.311540</td>\n",
       "      <td>37810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.914989</td>\n",
       "      <td>0.195549</td>\n",
       "      <td>0.869467</td>\n",
       "      <td>0.320838</td>\n",
       "      <td>36354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dl  fl1  fl2  fl3  kl  layer optimizer  train_acc  train_loss   val_acc  \\\n",
       "408   16  128   16   32   5      1      Adam   0.921803    0.181132  0.901725   \n",
       "445   64  128   16   64   5      2      Adam   0.917302    0.191708  0.902476   \n",
       "395   32  128   16   16   5      3     Nadam   0.918865    0.190353  0.900975   \n",
       "232   64   64   64   32   5      2     Nadam   0.915677    0.195752  0.895224   \n",
       "406  128  128   16   16   5      2     Nadam   0.918740    0.188945  0.899725   \n",
       "405  128  128   16   16   5      1     Nadam   0.914739    0.192709  0.896474   \n",
       "402  128  128   16   16   5      1      Adam   0.921053    0.181003  0.899725   \n",
       "336   16   64  128   64   5      1      Adam   0.919240    0.189732  0.899725   \n",
       "384   16  128   16   16   5      1      Adam   0.922240    0.178402  0.901475   \n",
       "441   32  128   16   64   5      1     Nadam   0.914927    0.196061  0.898225   \n",
       "0     16   64   16   16   5      1      Adam   0.916302    0.193030  0.894974   \n",
       "359  128   64  128   64   5      3     Nadam   0.914614    0.200537  0.893473   \n",
       "425   64  128   16   32   5      3     Nadam   0.918927    0.192746  0.896224   \n",
       "461   16  128   16  128   5      3     Nadam   0.916177    0.197497  0.897224   \n",
       "377   64   64  128  128   5      3     Nadam   0.914739    0.201833  0.897724   \n",
       "373   64   64  128  128   5      2      Adam   0.916865    0.194126  0.897974   \n",
       "487   32  128   32   16   5      2      Adam   0.919240    0.185281  0.901725   \n",
       "139  128   64   32   32   5      2      Adam   0.917740    0.192390  0.901475   \n",
       "470   64  128   16  128   5      3      Adam   0.916865    0.194695  0.898475   \n",
       "170   16   64   32  128   5      3      Adam   0.913864    0.199296  0.899475   \n",
       "326   64   64  128   32   5      3      Adam   0.915114    0.199119  0.897474   \n",
       "484   16  128   32   16   5      2     Nadam   0.918115    0.187847  0.898725   \n",
       "220   16   64   64   32   5      2     Nadam   0.915239    0.195792  0.897474   \n",
       "216   16   64   64   32   5      1      Adam   0.918365    0.187659  0.899975   \n",
       "288   16   64  128   16   5      1      Adam   0.918677    0.189671  0.899475   \n",
       "228   64   64   64   32   5      1      Adam   0.918802    0.188087  0.900725   \n",
       "101   16   64   32   16   5      3     Nadam   0.914302    0.202921  0.893473   \n",
       "386   16  128   16   16   5      3      Adam   0.917427    0.191698  0.898225   \n",
       "36    64   64   16   32   5      1      Adam   0.918177    0.188066  0.897474   \n",
       "90   128   64   16  128   5      1      Adam   0.917115    0.188769  0.895974   \n",
       "..   ...  ...  ...  ...  ..    ...       ...        ...         ...       ...   \n",
       "42   128   64   16   32   5      1      Adam   0.917615    0.187601  0.889972   \n",
       "129   32   64   32   32   5      1     Nadam   0.914489    0.199242  0.883471   \n",
       "196   16   64   64   16   5      2     Nadam   0.916615    0.191289  0.886972   \n",
       "412   16  128   16   32   5      2     Nadam   0.916302    0.194880  0.886722   \n",
       "324   64   64  128   32   5      1      Adam   0.918427    0.191462  0.887722   \n",
       "320   32   64  128   32   5      3      Adam   0.915364    0.194344  0.883471   \n",
       "464   32  128   16  128   5      3      Adam   0.915927    0.191898  0.892473   \n",
       "161   64   64   32   64   5      3     Nadam   0.914114    0.202205  0.891973   \n",
       "244   16   64   64   64   5      2     Nadam   0.915489    0.196842  0.871718   \n",
       "410   16  128   16   32   5      3      Adam   0.919052    0.189296  0.883971   \n",
       "51    16   64   16   64   5      1     Nadam   0.914864    0.196835  0.890973   \n",
       "212  128   64   64   16   5      3      Adam   0.914239    0.198099  0.881220   \n",
       "333  128   64  128   32   5      1     Nadam   0.911989    0.204128  0.883971   \n",
       "389   16  128   16   16   5      3     Nadam   0.916302    0.189844  0.891973   \n",
       "468   64  128   16  128   5      1      Adam   0.921553    0.177740  0.883221   \n",
       "133   64   64   32   32   5      2      Adam   0.917552    0.193023  0.880220   \n",
       "356  128   64  128   64   5      3      Adam   0.915114    0.198183  0.877969   \n",
       "99    16   64   32   16   5      1     Nadam   0.915614    0.196222  0.877969   \n",
       "294   32   64  128   16   5      1      Adam   0.916302    0.191133  0.880970   \n",
       "57    32   64   16   64   5      1     Nadam   0.915302    0.193594  0.876719   \n",
       "2     16   64   16   16   5      3      Adam   0.916177    0.195439  0.884221   \n",
       "213  128   64   64   16   5      1     Nadam   0.910864    0.204749  0.877219   \n",
       "322   32   64  128   32   5      2     Nadam   0.915114    0.195504  0.883221   \n",
       "318   32   64  128   32   5      1      Adam   0.919677    0.187814  0.877719   \n",
       "475  128  128   16  128   5      2      Adam   0.919740    0.187301  0.876969   \n",
       "13    64   64   16   16   5      2      Adam   0.918927    0.187650  0.883971   \n",
       "119  128   64   32   16   5      3     Nadam   0.913864    0.200447  0.876469   \n",
       "463   32  128   16  128   5      2      Adam   0.920615    0.180782  0.877969   \n",
       "7     32   64   16   16   5      2      Adam   0.916365    0.194129  0.877469   \n",
       "15    64   64   16   16   5      1     Nadam   0.914989    0.195549  0.869467   \n",
       "\n",
       "     val_loss  trainable_params  \n",
       "408  0.230709             66226  \n",
       "445  0.231941             75602  \n",
       "395  0.233088             76290  \n",
       "232  0.233315             56898  \n",
       "406  0.234236             76818  \n",
       "405  0.234565             80898  \n",
       "402  0.235206             80898  \n",
       "336  0.235302             33138  \n",
       "384  0.235378             66226  \n",
       "441  0.235445             68322  \n",
       "0    0.236199             33138  \n",
       "359  0.236664            122754  \n",
       "425  0.236727             79218  \n",
       "461  0.237220             86850  \n",
       "377  0.237753            163586  \n",
       "373  0.238680             81538  \n",
       "487  0.238722             85762  \n",
       "139  0.238763             46818  \n",
       "470  0.238928             93138  \n",
       "170  0.238976             65042  \n",
       "326  0.239169             95906  \n",
       "484  0.239315             85202  \n",
       "220  0.239663             53682  \n",
       "216  0.240112             33138  \n",
       "288  0.240734             33138  \n",
       "228  0.241033             36354  \n",
       "101  0.241495             45218  \n",
       "386  0.241539             75986  \n",
       "36   0.241607             36354  \n",
       "90   0.241705             40642  \n",
       "..        ...               ...  \n",
       "42   0.281464             40642  \n",
       "129  0.281938             34210  \n",
       "196  0.282142             53682  \n",
       "412  0.282329             74690  \n",
       "324  0.282447             36354  \n",
       "320  0.283351             94786  \n",
       "464  0.284476             88946  \n",
       "161  0.286213             56930  \n",
       "244  0.287318             53682  \n",
       "410  0.288285             77538  \n",
       "51   0.289141             33138  \n",
       "212  0.290793             60178  \n",
       "333  0.291366             40642  \n",
       "389  0.292030             75986  \n",
       "468  0.292098             72514  \n",
       "133  0.294988             44578  \n",
       "356  0.295547            122754  \n",
       "99   0.295568             33138  \n",
       "294  0.295794             34210  \n",
       "57   0.298285             34210  \n",
       "2    0.298465             38802  \n",
       "213  0.299356             40642  \n",
       "322  0.299551             77346  \n",
       "318  0.300084             34210  \n",
       "475  0.301659             76818  \n",
       "13   0.302694             38418  \n",
       "119  0.306944             47346  \n",
       "463  0.310799             74994  \n",
       "7    0.311540             37810  \n",
       "15   0.320838             36354  \n",
       "\n",
       "[495 rows x 12 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = pd.read_csv('embeddings_models.csv')\n",
    "embeddings_sort = embeddings.sort_values(['val_loss'])\n",
    "embeddings_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
