{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing function from a different ipny \n",
    "import pandas as pd\n",
    "import collections\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import operator\n",
    "from itertools import product\n",
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from functools import reduce\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "pd.options.display.max_columns = 1000\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Input, BatchNormalization, MaxPooling1D, Bidirectional,LSTM\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPool1D, Flatten , Embedding, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "#https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NUM_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>isspam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Confidential :Soma:, Ci@lis, :P:ntermin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¢Â ÇªÁüÇÑ ¼³³¯ ÀÌº¥Æ® ÀÀ¸ðÇØ¼­ ºÎ¸ð´Ô²² ¼±¹°ÇÏ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_na_</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>StOck 0ppurtunities - their sh0Oting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>All your prescr[iption needs right here</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Subject  isspam\n",
       "0            Confidential :Soma:, Ci@lis, :P:ntermin       1\n",
       "1  ¢Â ÇªÁüÇÑ ¼³³¯ ÀÌº¥Æ® ÀÀ¸ðÇØ¼­ ºÎ¸ð´Ô²² ¼±¹°ÇÏ...       1\n",
       "2                                               _na_       1\n",
       "3               StOck 0ppurtunities - their sh0Oting       1\n",
       "6            All your prescr[iption needs right here       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_data = pd.read_csv('subject_spam.csv', index_col=0, encoding='utf8', engine='python')\n",
    "keras_data.fillna(\"_na_\", inplace = True)\n",
    "keras_data = keras_data.drop(keras_data[keras_data.isspam == \"_na_\"].index)\n",
    "spammer = {'spam ': 1,'ham ': 0} \n",
    "keras_data.isspam = [spammer[item] for item in keras_data.isspam] \n",
    "spam_index = keras_data[keras_data.isspam ==1].index\n",
    "ham_index = keras_data[keras_data.isspam == 0].index\n",
    "new_index = np.concatenate((spam_index[:10000], ham_index[:9997]), axis=0)\n",
    "keras_data_new = keras_data.iloc[new_index]\n",
    "labels = keras_data_new.isspam\n",
    "keras_data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join('glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15425 unique tokens.\n",
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(keras_data_new.Subject)\n",
    "sequences = tokenizer.texts_to_sequences(keras_data_new.Subject)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples].reshape(15998,1000,1)\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:].reshape(3999,1000,1)\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print('Training model.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_embedding(fl1=16, fl2=16, fl3=16, dl=16, optimizer= 'RMSprop', kl = 5, layer =1 ):\n",
    "    inp =  Input(shape=(1000, 1))\n",
    "    if (layer == 1):\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(inp)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "    elif (layer == 2):\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(inp)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl2, kernel_size = kl, activation='relu')(x)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "    else:\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(inp)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl2, kernel_size = kl, activation='relu')(x)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl3, kernel_size = kl, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(units = dl, activation='relu')(x)\n",
    "    preds = Dense(1, activation='tanh')(x)\n",
    "    model = Model(inp, preds)\n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "              optimizer= optimizer,\n",
    "              metrics=['acc'])\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kera_trainable_params(model):\n",
    "    trainable_count = int(np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
    "    non_trainable_count = int(np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])) \n",
    "    return trainable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_edit(out):\n",
    "    loss_list = [s for s in out.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in out.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in out.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in out.history.keys() if 'acc' in s and 'val' in s]\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    for l in loss_list:\n",
    "        train_loss .append(out.history[l])\n",
    "    for l in val_loss_list:\n",
    "        val_loss .append(out.history[l])\n",
    "    for l in acc_list:\n",
    "        train_acc.append(out.history[l])\n",
    "    for l in val_acc_list:\n",
    "        val_acc .append(out.history[l])\n",
    "    line  = { 'train_loss': reduce(operator.concat, train_loss)[-1],'val_loss': reduce(operator.concat, val_loss)[-1], \\\n",
    "             'train_acc': reduce(operator.concat, train_acc)[-1], 'val_acc': reduce(operator.concat, val_acc)[-1] }\n",
    "    return (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 10s 635us/step - loss: 8.1733 - acc: 0.1121 - val_loss: 8.1699 - val_acc: 0.0545\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 8s 499us/step - loss: 8.2716 - acc: 0.0504 - val_loss: 8.1699 - val_acc: 0.0545\n",
      "{'train_loss': 8.27163170385188, 'val_loss': 8.169887369470198, 'train_acc': 0.050381297665002094, 'val_acc': 0.05451362843318533}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 10s 626us/step - loss: 3.7423 - acc: 0.3288 - val_loss: 2.3315 - val_acc: 0.4366\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 9s 579us/step - loss: 2.2760 - acc: 0.4090 - val_loss: 2.6063 - val_acc: 0.3958\n",
      "{'train_loss': 2.2760136232687276, 'val_loss': 2.6062887793751055, 'train_acc': 0.408988623581673, 'val_acc': 0.39584896234489436}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 11s 674us/step - loss: 6.0014 - acc: 0.2210 - val_loss: 4.0909 - val_acc: 0.2953\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 10s 619us/step - loss: 1.6344 - acc: 0.4736 - val_loss: 0.7753 - val_acc: 0.5046\n",
      "{'train_loss': 1.6343657252773343, 'val_loss': 0.7753150329258359, 'train_acc': 0.4736217027128391, 'val_acc': 0.504626156576397}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 9s 580us/step - loss: 8.2731 - acc: 0.0503 - val_loss: 8.1699 - val_acc: 0.0543\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 9s 536us/step - loss: 8.2716 - acc: 0.0502 - val_loss: 8.1699 - val_acc: 0.0543\n",
      "{'train_loss': 8.271631715595461, 'val_loss': 8.169887369470198, 'train_acc': 0.05019377422457204, 'val_acc': 0.05426356591755642}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 11s 657us/step - loss: 7.6129 - acc: 0.0719 - val_loss: 7.5662 - val_acc: 0.0710\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 9s 592us/step - loss: 7.6066 - acc: 0.0648 - val_loss: 7.5666 - val_acc: 0.0710\n",
      "{'train_loss': 7.6066233198230515, 'val_loss': 7.56655521111418, 'train_acc': 0.06475809476743387, 'val_acc': 0.0710177544646932}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 11s 704us/step - loss: 5.9475 - acc: 0.2333 - val_loss: 6.4271 - val_acc: 0.1553\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 10s 637us/step - loss: 5.9300 - acc: 0.2488 - val_loss: 5.2205 - val_acc: 0.3716\n",
      "{'train_loss': 5.929983938555521, 'val_loss': 5.220521699729399, 'train_acc': 0.2488436054618586, 'val_acc': 0.3715928982469135}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 10s 608us/step - loss: 7.6563 - acc: 0.0975 - val_loss: 7.6961 - val_acc: 0.0543\n",
      "Epoch 2/2\n",
      " 6560/15998 [===========>..................] - ETA: 4s - loss: 7.7869 - acc: 0.0466"
     ]
    }
   ],
   "source": [
    "# bounds for hyper-parameters in mnist model\n",
    "# the bounds dict should be in order of continuous type and then discrete type\n",
    "# create model\n",
    "# define the grid search parameters\n",
    "layers = [1,2,3]\n",
    "fl1s = [16,32,64, 128]\n",
    "fl2s = [16,32,64, 128]\n",
    "fl3s = [16,32,64,128]\n",
    "kls = [5]\n",
    "dls = [16,32,64,128]\n",
    "#sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "optimizers = ['Nadam', 'Adam', 'RMSprop', 'SGD'] #, 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "loss_data = []\n",
    "params = []\n",
    "trainable_count = []\n",
    "for fl1, fl2, fl3, kl, dl, optimizer,layer in product(fl1s,fl2s,fl3s,kls,dls,optimizers,layers):\n",
    "    kwargs = dict(fl1=fl1, fl2= fl2, fl3=fl3, kl=kl, dl=dl, optimizer= ''.join(optimizer), layer=layer)\n",
    "    params.append(kwargs)\n",
    "    model = no_embedding(**kwargs)\n",
    "    trainable_count.append(kera_trainable_params(model))\n",
    "    history = model.fit(x_train, y_train, batch_size= 16, epochs=2, validation_data=(x_val, y_val))\n",
    "    loss_data.append(loss_edit(history))\n",
    "    print(loss_edit(history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_pd = pd.DataFrame(params)\n",
    "loss_pd= pd.DataFrame(loss_data)\n",
    "trainable_pd = pd.DataFrame({'trainable_params': trainable_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_embeddings = pd.concat([params_pd, loss_pd, trainable_pd], axis=1, join='inner')\n",
    "no_embeddings.to_csv('no_embeddings_models.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dl</th>\n",
       "      <th>fl1</th>\n",
       "      <th>fl2</th>\n",
       "      <th>fl3</th>\n",
       "      <th>kl</th>\n",
       "      <th>layer</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>trainable_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.572384</td>\n",
       "      <td>0.673456</td>\n",
       "      <td>0.576144</td>\n",
       "      <td>0.661850</td>\n",
       "      <td>5106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.563133</td>\n",
       "      <td>0.670764</td>\n",
       "      <td>0.570393</td>\n",
       "      <td>0.663311</td>\n",
       "      <td>26898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.558320</td>\n",
       "      <td>0.687657</td>\n",
       "      <td>0.569142</td>\n",
       "      <td>0.663556</td>\n",
       "      <td>30114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.570259</td>\n",
       "      <td>0.665551</td>\n",
       "      <td>0.562641</td>\n",
       "      <td>0.665051</td>\n",
       "      <td>4546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.569884</td>\n",
       "      <td>0.679560</td>\n",
       "      <td>0.586397</td>\n",
       "      <td>0.669365</td>\n",
       "      <td>16114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.576072</td>\n",
       "      <td>0.675641</td>\n",
       "      <td>0.582896</td>\n",
       "      <td>0.669398</td>\n",
       "      <td>27490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.545631</td>\n",
       "      <td>0.679958</td>\n",
       "      <td>0.560140</td>\n",
       "      <td>0.669625</td>\n",
       "      <td>6482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.554444</td>\n",
       "      <td>0.670132</td>\n",
       "      <td>0.559890</td>\n",
       "      <td>0.669899</td>\n",
       "      <td>10082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.554882</td>\n",
       "      <td>0.674300</td>\n",
       "      <td>0.570893</td>\n",
       "      <td>0.669979</td>\n",
       "      <td>6354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.566571</td>\n",
       "      <td>0.680231</td>\n",
       "      <td>0.587897</td>\n",
       "      <td>0.670605</td>\n",
       "      <td>2994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.547881</td>\n",
       "      <td>0.692597</td>\n",
       "      <td>0.573893</td>\n",
       "      <td>0.670869</td>\n",
       "      <td>10722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.573447</td>\n",
       "      <td>0.670578</td>\n",
       "      <td>0.568642</td>\n",
       "      <td>0.673221</td>\n",
       "      <td>8402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.566008</td>\n",
       "      <td>0.690125</td>\n",
       "      <td>0.578145</td>\n",
       "      <td>0.673663</td>\n",
       "      <td>8402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.561570</td>\n",
       "      <td>0.672676</td>\n",
       "      <td>0.557389</td>\n",
       "      <td>0.674488</td>\n",
       "      <td>5122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.565821</td>\n",
       "      <td>0.688709</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.674620</td>\n",
       "      <td>8962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.547443</td>\n",
       "      <td>0.676523</td>\n",
       "      <td>0.532133</td>\n",
       "      <td>0.675534</td>\n",
       "      <td>3298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.559632</td>\n",
       "      <td>0.709371</td>\n",
       "      <td>0.579145</td>\n",
       "      <td>0.675731</td>\n",
       "      <td>6354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.544818</td>\n",
       "      <td>0.680838</td>\n",
       "      <td>0.554889</td>\n",
       "      <td>0.675903</td>\n",
       "      <td>11026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.578010</td>\n",
       "      <td>0.682099</td>\n",
       "      <td>0.591148</td>\n",
       "      <td>0.676104</td>\n",
       "      <td>14066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.552257</td>\n",
       "      <td>0.683877</td>\n",
       "      <td>0.508877</td>\n",
       "      <td>0.676352</td>\n",
       "      <td>34402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.566633</td>\n",
       "      <td>0.784337</td>\n",
       "      <td>0.566142</td>\n",
       "      <td>0.676722</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.542568</td>\n",
       "      <td>0.687695</td>\n",
       "      <td>0.546887</td>\n",
       "      <td>0.677015</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.576822</td>\n",
       "      <td>0.673783</td>\n",
       "      <td>0.573393</td>\n",
       "      <td>0.677533</td>\n",
       "      <td>48466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.560008</td>\n",
       "      <td>0.680698</td>\n",
       "      <td>0.566392</td>\n",
       "      <td>0.677538</td>\n",
       "      <td>8962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.553944</td>\n",
       "      <td>0.682083</td>\n",
       "      <td>0.565891</td>\n",
       "      <td>0.678442</td>\n",
       "      <td>16674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.551256</td>\n",
       "      <td>0.697660</td>\n",
       "      <td>0.557139</td>\n",
       "      <td>0.678639</td>\n",
       "      <td>8962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.566383</td>\n",
       "      <td>0.675193</td>\n",
       "      <td>0.570643</td>\n",
       "      <td>0.678646</td>\n",
       "      <td>15954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.524503</td>\n",
       "      <td>0.697759</td>\n",
       "      <td>0.541135</td>\n",
       "      <td>0.678693</td>\n",
       "      <td>3826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>0.564633</td>\n",
       "      <td>0.689849</td>\n",
       "      <td>0.566642</td>\n",
       "      <td>0.679297</td>\n",
       "      <td>5122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.550569</td>\n",
       "      <td>0.684362</td>\n",
       "      <td>0.560640</td>\n",
       "      <td>0.679466</td>\n",
       "      <td>30114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808419</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665126</td>\n",
       "      <td>3826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808403</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665127</td>\n",
       "      <td>4930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808404</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665129</td>\n",
       "      <td>7426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808432</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665129</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.490186</td>\n",
       "      <td>7.716671</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665130</td>\n",
       "      <td>7650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808425</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665131</td>\n",
       "      <td>17794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808430</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665131</td>\n",
       "      <td>1314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808411</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665131</td>\n",
       "      <td>1314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808431</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665133</td>\n",
       "      <td>7170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808433</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665133</td>\n",
       "      <td>1314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808431</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665134</td>\n",
       "      <td>2530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808438</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665135</td>\n",
       "      <td>2530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808435</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665136</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808438</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665137</td>\n",
       "      <td>3826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808432</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665137</td>\n",
       "      <td>1314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808441</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665139</td>\n",
       "      <td>2530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808436</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665139</td>\n",
       "      <td>3826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808445</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665142</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808420</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665145</td>\n",
       "      <td>7170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808454</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665172</td>\n",
       "      <td>2610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.488624</td>\n",
       "      <td>7.761327</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665173</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808646</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665291</td>\n",
       "      <td>7426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808685</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665311</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>7.808933</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.665415</td>\n",
       "      <td>7170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.523815</td>\n",
       "      <td>7.111856</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>7.687877</td>\n",
       "      <td>2530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.513189</td>\n",
       "      <td>7.846464</td>\n",
       "      <td>0.506877</td>\n",
       "      <td>7.948208</td>\n",
       "      <td>2610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.513189</td>\n",
       "      <td>7.846464</td>\n",
       "      <td>0.506877</td>\n",
       "      <td>7.948208</td>\n",
       "      <td>13858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.513189</td>\n",
       "      <td>7.846464</td>\n",
       "      <td>0.506877</td>\n",
       "      <td>7.948208</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>8.271632</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>8.169887</td>\n",
       "      <td>26898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.486811</td>\n",
       "      <td>8.271632</td>\n",
       "      <td>0.493123</td>\n",
       "      <td>8.169887</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>541 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dl  fl1  fl2  fl3  kl  layer optimizer  train_acc  train_loss   val_acc  \\\n",
       "71    32   16   16   32   5      3     Nadam   0.572384    0.673456  0.576144   \n",
       "491   16   16   64   64   5      3     Nadam   0.563133    0.670764  0.570393   \n",
       "515   64   16   64   64   5      3     Nadam   0.558320    0.687657  0.569142   \n",
       "53    16   16   16   32   5      3      Adam   0.570259    0.665551  0.562641   \n",
       "437   16   16   64   32   5      3      Adam   0.569884    0.679560  0.586397   \n",
       "353   32   16   32  128   5      3      Adam   0.576072    0.675641  0.582896   \n",
       "221   64   16   32   16   5      3      Adam   0.545631    0.679958  0.560140   \n",
       "275   64   16   32   32   5      3     Nadam   0.554444    0.670132  0.559890   \n",
       "442   16   16   64   32   5      2     Nadam   0.554882    0.674300  0.570893   \n",
       "2     16   16   16   16   5      3   RMSprop   0.566571    0.680231  0.587897   \n",
       "389   16   16   64   16   5      3      Adam   0.547881    0.692597  0.573893   \n",
       "251   16   16   32   32   5      3     Nadam   0.573447    0.670578  0.568642   \n",
       "245   16   16   32   32   5      3      Adam   0.566008    0.690125  0.578145   \n",
       "41   128   16   16   16   5      3      Adam   0.561570    0.672676  0.557389   \n",
       "257   32   16   32   32   5      3      Adam   0.565821    0.688709  0.569892   \n",
       "23    32   16   16   16   5      3     Nadam   0.547443    0.676523  0.532133   \n",
       "529   16   16   64  128   5      2   RMSprop   0.559632    0.709371  0.579145   \n",
       "401   32   16   64   16   5      3      Adam   0.544818    0.680838  0.554889   \n",
       "290   16   16   32   64   5      3   RMSprop   0.578010    0.682099  0.591148   \n",
       "521  128   16   64   64   5      3      Adam   0.552257    0.683877  0.508877   \n",
       "435   16   16   64   32   5      1      Adam   0.566633    0.784337  0.566142   \n",
       "297   16   16   32   64   5      1     Nadam   0.542568    0.687695  0.546887   \n",
       "533   16   16   64  128   5      3      Adam   0.576822    0.673783  0.573393   \n",
       "263   32   16   32   32   5      3     Nadam   0.560008    0.680698  0.566392   \n",
       "449   32   16   64   32   5      3      Adam   0.553944    0.682083  0.565891   \n",
       "254   32   16   32   32   5      3   RMSprop   0.551256    0.697660  0.557139   \n",
       "158   32   16   16  128   5      3   RMSprop   0.566383    0.675193  0.570643   \n",
       "40   128   16   16   16   5      2      Adam   0.524503    0.697759  0.541135   \n",
       "38   128   16   16   16   5      3   RMSprop   0.564633    0.689849  0.566642   \n",
       "509   64   16   64   64   5      3      Adam   0.550569    0.684362  0.560640   \n",
       "..   ...  ...  ...  ...  ..    ...       ...        ...         ...       ...   \n",
       "139  128   16   16   64   5      2       SGD   0.486811    7.808419  0.493123   \n",
       "367   64   16   32  128   5      2       SGD   0.486811    7.808403  0.493123   \n",
       "499   32   16   64   64   5      2       SGD   0.486811    7.808404  0.493123   \n",
       "54    16   16   16   32   5      1       SGD   0.486811    7.808432  0.493123   \n",
       "104   16   16   16   64   5      3       SGD   0.490186    7.716671  0.493123   \n",
       "464   64   16   64   32   5      3       SGD   0.486811    7.808425  0.493123   \n",
       "462   64   16   64   32   5      1       SGD   0.486811    7.808430  0.493123   \n",
       "318   64   16   32   64   5      1       SGD   0.486811    7.808411  0.493123   \n",
       "331  128   16   32   64   5      2       SGD   0.486811    7.808431  0.493123   \n",
       "414   64   16   64   16   5      1       SGD   0.486811    7.808433  0.493123   \n",
       "282  128   16   32   32   5      1       SGD   0.486811    7.808431  0.493123   \n",
       "186  128   16   16  128   5      1       SGD   0.486811    7.808438  0.493123   \n",
       "294   16   16   32   64   5      1       SGD   0.486811    7.808435  0.493123   \n",
       "43   128   16   16   16   5      2       SGD   0.486811    7.808438  0.493123   \n",
       "222   64   16   32   16   5      1       SGD   0.486811    7.808432  0.493123   \n",
       "42   128   16   16   16   5      1       SGD   0.486811    7.808441  0.493123   \n",
       "187  128   16   16  128   5      2       SGD   0.486811    7.808436  0.493123   \n",
       "258   32   16   32   32   5      1       SGD   0.486811    7.808445  0.493123   \n",
       "235  128   16   32   16   5      2       SGD   0.486811    7.808420  0.493123   \n",
       "175   64   16   16  128   5      2       SGD   0.486811    7.808454  0.493123   \n",
       "534   16   16   64  128   5      1       SGD   0.488624    7.761327  0.493123   \n",
       "451   32   16   64   32   5      2       SGD   0.486811    7.808646  0.493123   \n",
       "150   16   16   16  128   5      1       SGD   0.486811    7.808685  0.493123   \n",
       "379  128   16   32  128   5      2       SGD   0.486811    7.808933  0.493123   \n",
       "141  128   16   16   64   5      1     Nadam   0.523815    7.111856  0.493123   \n",
       "79    64   16   16   32   5      2       SGD   0.513189    7.846464  0.506877   \n",
       "523  128   16   64   64   5      2       SGD   0.513189    7.846464  0.506877   \n",
       "66    32   16   16   32   5      1       SGD   0.513189    7.846464  0.506877   \n",
       "488   16   16   64   64   5      3       SGD   0.486811    8.271632  0.493123   \n",
       "114   32   16   16   64   5      1       SGD   0.486811    8.271632  0.493123   \n",
       "\n",
       "     val_loss  trainable_params  \n",
       "71   0.661850              5106  \n",
       "491  0.663311             26898  \n",
       "515  0.663556             30114  \n",
       "53   0.665051              4546  \n",
       "437  0.669365             16114  \n",
       "353  0.669398             27490  \n",
       "221  0.669625              6482  \n",
       "275  0.669899             10082  \n",
       "442  0.669979              6354  \n",
       "2    0.670605              2994  \n",
       "389  0.670869             10722  \n",
       "251  0.673221              8402  \n",
       "245  0.673663              8402  \n",
       "41   0.674488              5122  \n",
       "257  0.674620              8962  \n",
       "23   0.675534              3298  \n",
       "529  0.675731              6354  \n",
       "401  0.675903             11026  \n",
       "290  0.676104             14066  \n",
       "521  0.676352             34402  \n",
       "435  0.676722               402  \n",
       "297  0.677015               402  \n",
       "533  0.677533             48466  \n",
       "263  0.677538              8962  \n",
       "449  0.678442             16674  \n",
       "254  0.678639              8962  \n",
       "158  0.678646             15954  \n",
       "40   0.678693              3826  \n",
       "38   0.679297              5122  \n",
       "509  0.679466             30114  \n",
       "..        ...               ...  \n",
       "139  7.665126              3826  \n",
       "367  7.665127              4930  \n",
       "499  7.665129              7426  \n",
       "54   7.665129               402  \n",
       "104  7.665130              7650  \n",
       "464  7.665131             17794  \n",
       "462  7.665131              1314  \n",
       "318  7.665131              1314  \n",
       "331  7.665133              7170  \n",
       "414  7.665133              1314  \n",
       "282  7.665134              2530  \n",
       "186  7.665135              2530  \n",
       "294  7.665136               402  \n",
       "43   7.665137              3826  \n",
       "222  7.665137              1314  \n",
       "42   7.665139              2530  \n",
       "187  7.665139              3826  \n",
       "258  7.665142               706  \n",
       "235  7.665145              7170  \n",
       "175  7.665172              2610  \n",
       "534  7.665173               402  \n",
       "451  7.665291              7426  \n",
       "150  7.665311               402  \n",
       "379  7.665415              7170  \n",
       "141  7.687877              2530  \n",
       "79   7.948208              2610  \n",
       "523  7.948208             13858  \n",
       "66   7.948208               706  \n",
       "488  8.169887             26898  \n",
       "114  8.169887               706  \n",
       "\n",
       "[541 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_embeddings = pd.read_csv('no_embeddings_models.csv')\n",
    "no_embeddings_sort = no_embeddings.sort_values(['val_loss'])\n",
    "no_embeddings_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
