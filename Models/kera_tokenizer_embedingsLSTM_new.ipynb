{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/titli/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing function from a different ipny \n",
    "import pandas as pd\n",
    "import collections\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import operator\n",
    "from itertools import product\n",
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from functools import reduce\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "pd.options.display.max_columns = 1000\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Input, BatchNormalization, MaxPooling1D, Bidirectional,LSTM\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPool1D, Flatten , Embedding, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#https://www.amazon.com/Neural-Networks-Deep-Learning-Textbook/dp/3319944622/ref=cm_cr_arp_d_product_top?ie=UTF8\n",
    "#https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NUM_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>isspam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Confidential :Soma:, Ci@lis, :P:ntermin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¢Â ÇªÁüÇÑ ¼³³¯ ÀÌº¥Æ® ÀÀ¸ðÇØ¼­ ºÎ¸ð´Ô²² ¼±¹°ÇÏ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_na_</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>StOck 0ppurtunities - their sh0Oting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>All your prescr[iption needs right here</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Subject  isspam\n",
       "0            Confidential :Soma:, Ci@lis, :P:ntermin       1\n",
       "1  ¢Â ÇªÁüÇÑ ¼³³¯ ÀÌº¥Æ® ÀÀ¸ðÇØ¼­ ºÎ¸ð´Ô²² ¼±¹°ÇÏ...       1\n",
       "2                                               _na_       1\n",
       "3               StOck 0ppurtunities - their sh0Oting       1\n",
       "6            All your prescr[iption needs right here       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_data = pd.read_csv('subject_spam.csv', index_col=0, encoding='utf8', engine='python')\n",
    "keras_data.fillna(\"_na_\", inplace = True)\n",
    "keras_data = keras_data.drop(keras_data[keras_data.isspam == \"_na_\"].index)\n",
    "spammer = {'spam ': 1,'ham ': 0} \n",
    "keras_data.isspam = [spammer[item] for item in keras_data.isspam] \n",
    "spam_index = keras_data[keras_data.isspam ==1].index\n",
    "ham_index = keras_data[keras_data.isspam == 0].index\n",
    "new_index = np.concatenate((spam_index[:10000], ham_index[:9997]), axis=0)\n",
    "keras_data_new = keras_data.iloc[new_index]\n",
    "labels = keras_data_new.isspam\n",
    "keras_data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join('glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15425 unique tokens.\n",
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(keras_data_new.Subject)\n",
    "sequences = tokenizer.texts_to_sequences(keras_data_new.Subject)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print('Training model.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_edit(out):\n",
    "    loss_list = [s for s in out.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in out.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in out.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in out.history.keys() if 'acc' in s and 'val' in s]\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    for l in loss_list:\n",
    "        train_loss .append(out.history[l])\n",
    "    for l in val_loss_list:\n",
    "        val_loss .append(out.history[l])\n",
    "    for l in acc_list:\n",
    "        train_acc.append(out.history[l])\n",
    "    for l in val_acc_list:\n",
    "        val_acc .append(out.history[l])\n",
    "    line  = { 'train_loss': reduce(operator.concat, train_loss)[-1],'val_loss': reduce(operator.concat, val_loss)[-1], \\\n",
    "             'train_acc': reduce(operator.concat, train_acc)[-1], 'val_acc': reduce(operator.concat, val_acc)[-1] }\n",
    "    return (line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_LSTM(fl1=16, fl2=16, fl3=16, dl=16, optimizer= 'RMSprop', kl = 5, layer =1): \n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Bidirectional(LSTM(units = fl1, return_sequences=True))(embedded_sequences)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(units=dl, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    preds = Dense(1, activation='tanh')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "              optimizer= optimizer,\n",
    "              metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 1046s 65ms/step - loss: 0.4481 - acc: 0.8097 - val_loss: 0.3518 - val_acc: 0.8637\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 1593s 100ms/step - loss: 0.3328 - acc: 0.8451 - val_loss: 0.3257 - val_acc: 0.8585\n",
      "{'train_loss': 0.3327841656316309, 'val_loss': 0.3257450566392566, 'train_acc': 0.845105638182421, 'val_acc': 0.8584646161838483}\n",
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "  160/15998 [..............................] - ETA: 33:23 - loss: 4.1760 - acc: 0.4938"
     ]
    }
   ],
   "source": [
    "layers = [1]\n",
    "fl1s = [16, 32, 64, 128]\n",
    "fl2s = [0]\n",
    "fl3s = [0]\n",
    "kls = [0]\n",
    "dls = [16,32,64,128]\n",
    "optimizers = ['Adam', 'Nadam', 'RMSprop', 'sgd']  \n",
    "loss_data = []\n",
    "params = []\n",
    "for fl1, fl2, fl3, kl, dl, optimizer,layer in product(fl1s,fl2s,fl3s,kls,dls,optimizers,layers):\n",
    "    kwargs = dict(fl1=fl1, fl2= fl2, fl3=fl3, kl=kl, dl=dl, optimizer= ''.join(optimizer), layer=layer)\n",
    "    params.append(kwargs)\n",
    "    model = embedding_LSTM(**kwargs)\n",
    "    #model.fit(x_train, y_train)\n",
    "    history = model.fit(x_train, y_train, batch_size= 16, epochs=2, validation_data=(x_val, y_val))\n",
    "    loss_data.append(loss_edit(history))\n",
    "    print(loss_edit(history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here an example model is shown. This type of analysis is done for many more models not shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_pd = pd.DataFrame(params)\n",
    "loss_pd= pd.DataFrame(loss_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_LSTM = pd.concat([params_pd, loss_pd], axis=1, join='inner')\n",
    "embeddings_LSTM.to_csv('embeddings_LSTM_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dl</th>\n",
       "      <th>fl1</th>\n",
       "      <th>fl2</th>\n",
       "      <th>fl3</th>\n",
       "      <th>kl</th>\n",
       "      <th>layer</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.931366</td>\n",
       "      <td>0.164316</td>\n",
       "      <td>0.928982</td>\n",
       "      <td>0.177332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.926553</td>\n",
       "      <td>0.180566</td>\n",
       "      <td>0.922731</td>\n",
       "      <td>0.187457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.933054</td>\n",
       "      <td>0.163492</td>\n",
       "      <td>0.926232</td>\n",
       "      <td>0.188538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.926491</td>\n",
       "      <td>0.177937</td>\n",
       "      <td>0.925231</td>\n",
       "      <td>0.189407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.928116</td>\n",
       "      <td>0.173650</td>\n",
       "      <td>0.923231</td>\n",
       "      <td>0.189689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.920678</td>\n",
       "      <td>0.195502</td>\n",
       "      <td>0.922731</td>\n",
       "      <td>0.190988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.923928</td>\n",
       "      <td>0.188712</td>\n",
       "      <td>0.923981</td>\n",
       "      <td>0.192867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.912677</td>\n",
       "      <td>0.216709</td>\n",
       "      <td>0.916979</td>\n",
       "      <td>0.193921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.930179</td>\n",
       "      <td>0.175449</td>\n",
       "      <td>0.916979</td>\n",
       "      <td>0.198198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.925803</td>\n",
       "      <td>0.182754</td>\n",
       "      <td>0.917979</td>\n",
       "      <td>0.201916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.915864</td>\n",
       "      <td>0.211497</td>\n",
       "      <td>0.917729</td>\n",
       "      <td>0.202676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.925241</td>\n",
       "      <td>0.185428</td>\n",
       "      <td>0.918730</td>\n",
       "      <td>0.203725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.920803</td>\n",
       "      <td>0.194990</td>\n",
       "      <td>0.915729</td>\n",
       "      <td>0.206063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.922490</td>\n",
       "      <td>0.195431</td>\n",
       "      <td>0.919730</td>\n",
       "      <td>0.212364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.911176</td>\n",
       "      <td>0.219128</td>\n",
       "      <td>0.912728</td>\n",
       "      <td>0.215874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.928179</td>\n",
       "      <td>0.173140</td>\n",
       "      <td>0.911228</td>\n",
       "      <td>0.216344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dl  fl1  fl2  fl3  kl  layer optimizer  train_acc  train_loss   val_acc  \\\n",
       "13   64  128    0    0   0      1     Nadam   0.931366    0.164316  0.928982   \n",
       "11   32  128    0    0   0      1     Nadam   0.926553    0.180566  0.922731   \n",
       "5    64   64    0    0   0      1     Nadam   0.933054    0.163492  0.926232   \n",
       "6   128   64    0    0   0      1      Adam   0.926491    0.177937  0.925231   \n",
       "14  128  128    0    0   0      1      Adam   0.928116    0.173650  0.923231   \n",
       "15  128  128    0    0   0      1     Nadam   0.920678    0.195502  0.922731   \n",
       "1    16   64    0    0   0      1     Nadam   0.923928    0.188712  0.923981   \n",
       "10   32  128    0    0   0      1      Adam   0.912677    0.216709  0.916979   \n",
       "3    32   64    0    0   0      1     Nadam   0.930179    0.175449  0.916979   \n",
       "4    64   64    0    0   0      1      Adam   0.925803    0.182754  0.917979   \n",
       "8    16  128    0    0   0      1      Adam   0.915864    0.211497  0.917729   \n",
       "7   128   64    0    0   0      1     Nadam   0.925241    0.185428  0.918730   \n",
       "0    16   64    0    0   0      1      Adam   0.920803    0.194990  0.915729   \n",
       "9    16  128    0    0   0      1     Nadam   0.922490    0.195431  0.919730   \n",
       "2    32   64    0    0   0      1      Adam   0.911176    0.219128  0.912728   \n",
       "12   64  128    0    0   0      1      Adam   0.928179    0.173140  0.911228   \n",
       "\n",
       "    val_loss  \n",
       "13  0.177332  \n",
       "11  0.187457  \n",
       "5   0.188538  \n",
       "6   0.189407  \n",
       "14  0.189689  \n",
       "15  0.190988  \n",
       "1   0.192867  \n",
       "10  0.193921  \n",
       "3   0.198198  \n",
       "4   0.201916  \n",
       "8   0.202676  \n",
       "7   0.203725  \n",
       "0   0.206063  \n",
       "9   0.212364  \n",
       "2   0.215874  \n",
       "12  0.216344  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_LSTM_sort = embeddings_LSTM.sort_values(['val_loss'])\n",
    "embeddings_LSTM_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
