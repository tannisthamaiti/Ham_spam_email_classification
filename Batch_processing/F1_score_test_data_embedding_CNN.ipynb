{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing function from a different ipny \n",
    "import pandas as pd\n",
    "import collections\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import operator\n",
    "from itertools import product\n",
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from functools import reduce\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "pd.options.display.max_columns = 1000\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Input, BatchNormalization, MaxPooling1D, Bidirectional,LSTM\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPool1D, Flatten , Embedding, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#https://www.amazon.com/Neural-Networks-Deep-Learning-Textbook/dp/3319944622/ref=cm_cr_arp_d_product_top?ie=UTF8\n",
    "#https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NUM_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>isspam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Confidential :Soma:, Ci@lis, :P:ntermin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¢Â ÇªÁüÇÑ ¼³³¯ ÀÌº¥Æ® ÀÀ¸ðÇØ¼­ ºÎ¸ð´Ô²² ¼±¹°ÇÏ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_na_</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>StOck 0ppurtunities - their sh0Oting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>All your prescr[iption needs right here</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Subject  isspam\n",
       "0            Confidential :Soma:, Ci@lis, :P:ntermin       1\n",
       "1  ¢Â ÇªÁüÇÑ ¼³³¯ ÀÌº¥Æ® ÀÀ¸ðÇØ¼­ ºÎ¸ð´Ô²² ¼±¹°ÇÏ...       1\n",
       "2                                               _na_       1\n",
       "3               StOck 0ppurtunities - their sh0Oting       1\n",
       "6            All your prescr[iption needs right here       1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_data = pd.read_csv('subject_spam.csv', index_col=0, encoding='utf8', engine='python')\n",
    "keras_data.fillna(\"_na_\", inplace = True)\n",
    "keras_data = keras_data.drop(keras_data[keras_data.isspam == \"_na_\"].index)\n",
    "spammer = {'spam ': 1,'ham ': 0} \n",
    "keras_data.isspam = [spammer[item] for item in keras_data.isspam] \n",
    "spam_index = keras_data[keras_data.isspam ==1].index\n",
    "ham_index = keras_data[keras_data.isspam == 0].index\n",
    "new_index = np.concatenate((spam_index[:10000], ham_index[:9997]), axis=0)\n",
    "keras_data_new = keras_data.iloc[new_index]\n",
    "labels = keras_data_new.isspam\n",
    "keras_data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join('glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15425 unique tokens.\n",
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(keras_data_new.Subject)\n",
    "sequences = tokenizer.texts_to_sequences(keras_data_new.Subject)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print('Training model.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>isspam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19223</th>\n",
       "      <td>RE: Daily Summary of Risk Data</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19224</th>\n",
       "      <td>Softwares CDS all software under 15$ and 99$!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19226</th>\n",
       "      <td>Hotel Room Bargains at up to 70% off!  Save in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19227</th>\n",
       "      <td>RE: Greeley Gas Company</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19228</th>\n",
       "      <td>Any Software just in under 15-99$, Xp-adobe etc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Subject  isspam\n",
       "19223                     RE: Daily Summary of Risk Data       0\n",
       "19224      Softwares CDS all software under 15$ and 99$!       1\n",
       "19226  Hotel Room Bargains at up to 70% off!  Save in...       1\n",
       "19227                            RE: Greeley Gas Company       0\n",
       "19228    Any Software just in under 15-99$, Xp-adobe etc       1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_data = pd.read_csv('subject_spam.csv', index_col=0, encoding='utf8', engine='python')\n",
    "keras_data.fillna(\"_na_\", inplace = True)\n",
    "keras_data = keras_data.drop(keras_data[keras_data.isspam == \"_na_\"].index)\n",
    "spammer = {'spam ': 1,'ham ': 0} \n",
    "keras_data.isspam = [spammer[item] for item in keras_data.isspam] \n",
    "spam_index = keras_data[keras_data.isspam ==1].index\n",
    "ham_index = keras_data[keras_data.isspam == 0].index\n",
    "new_index = np.concatenate((spam_index[10000:19000], ham_index[9997:19000]), axis=0)\n",
    "keras_data_test_set = keras_data.iloc[new_index]\n",
    "labels_test_set = keras_data_test_set.isspam\n",
    "keras_data_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15425 unique tokens.\n",
      "Shape of data tensor: (17088, 1000)\n",
      "Shape of label tensor: (19997,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(keras_data_new.Subject)\n",
    "sequences = tokenizer.texts_to_sequences(keras_data_test_set.Subject)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#labels_test_set = to_categorical(np.asarray(labels_test_set))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "#labels_test_set = labels_test_set[indices]\n",
    "x_test = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = pd.read_csv('embeddings_total.csv')\n",
    "result_sort = embedding.sort_values(['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dl</th>\n",
       "      <th>fl1</th>\n",
       "      <th>fl2</th>\n",
       "      <th>fl3</th>\n",
       "      <th>kl</th>\n",
       "      <th>layer</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>trainable_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.921803</td>\n",
       "      <td>0.181132</td>\n",
       "      <td>0.901725</td>\n",
       "      <td>0.230709</td>\n",
       "      <td>66226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.917302</td>\n",
       "      <td>0.191708</td>\n",
       "      <td>0.902476</td>\n",
       "      <td>0.231941</td>\n",
       "      <td>75602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.918865</td>\n",
       "      <td>0.190353</td>\n",
       "      <td>0.900975</td>\n",
       "      <td>0.233088</td>\n",
       "      <td>76290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.915677</td>\n",
       "      <td>0.195752</td>\n",
       "      <td>0.895224</td>\n",
       "      <td>0.233315</td>\n",
       "      <td>56898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.918740</td>\n",
       "      <td>0.188945</td>\n",
       "      <td>0.899725</td>\n",
       "      <td>0.234236</td>\n",
       "      <td>76818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dl  fl1  fl2  fl3  kl  layer optimizer  train_acc  train_loss  \\\n",
       "1989   16  128   16   32   5      1      Adam   0.921803    0.181132   \n",
       "2026   64  128   16   64   5      2      Adam   0.917302    0.191708   \n",
       "1976   32  128   16   16   5      3     Nadam   0.918865    0.190353   \n",
       "1813   64   64   64   32   5      2     Nadam   0.915677    0.195752   \n",
       "1987  128  128   16   16   5      2     Nadam   0.918740    0.188945   \n",
       "\n",
       "       val_acc  val_loss  trainable_params  \n",
       "1989  0.901725  0.230709             66226  \n",
       "2026  0.902476  0.231941             75602  \n",
       "1976  0.900975  0.233088             76290  \n",
       "1813  0.895224  0.233315             56898  \n",
       "1987  0.899725  0.234236             76818  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result = pd.concat([embedding1, embedding], axis=0)\n",
    "#result_sort = result.sort_values(['val_loss'])\n",
    "result_sort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(fl1=32, fl2=32, fl3=64, dl=16, optimizer= 'RMSprop', kl = 5, layer =1 ):\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    if (layer == 1):\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "    elif (layer == 2):\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl2, kernel_size = kl, activation='relu')(x)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        \n",
    "    else:\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl2, kernel_size = kl, activation='relu')(x)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl3, kernel_size = kl, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(units = dl, activation='relu')(x)\n",
    "    preds = Dense(1, activation='tanh')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "              optimizer= optimizer,\n",
    "              metrics=['acc'])\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 372s 23ms/step - loss: 0.5869 - acc: 0.6908 - val_loss: 0.5413 - val_acc: 0.6989\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 325s 20ms/step - loss: 0.4385 - acc: 0.7536 - val_loss: 0.5013 - val_acc: 0.7219\n",
      "Saved model to disk\n",
      "17088/17088 [==============================] - 131s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "kwargs = dict(fl1=128, fl2= 0, fl3=0, kl=5, dl=16, optimizer= ''.join('Adam'), layer=1)\n",
    "model = embeddings(**kwargs)\n",
    "model.fit(x_train, y_train, batch_size= 16, epochs=2, validation_data=(x_val, y_val))\n",
    "model_json = model.to_json()\n",
    "with open(\"model_embeddings_1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_embeddings_1.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "model1_test_y = model.predict(x_test, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 314s 20ms/step - loss: 0.5111 - acc: 0.7665 - val_loss: 0.4755 - val_acc: 0.7404\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 311s 19ms/step - loss: 0.4056 - acc: 0.7147 - val_loss: 0.4097 - val_acc: 0.7319\n",
      "Saved model to disk\n",
      "17088/17088 [==============================] - 134s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "kwargs = dict(fl1=128, fl2= 16, fl3=0, kl=5, dl=64, optimizer= ''.join('Adam'), layer=2)\n",
    "model = embeddings(**kwargs)\n",
    "model.fit(x_train, y_train, batch_size= 16, epochs=2, validation_data=(x_val, y_val))\n",
    "model_json = model.to_json()\n",
    "with open(\"model_embeddings_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_embeddings_2.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "model2_test_y = model.predict(x_test, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 319s 20ms/step - loss: 0.4482 - acc: 0.8016 - val_loss: 0.3709 - val_acc: 0.8652\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 312s 20ms/step - loss: 0.3344 - acc: 0.8572 - val_loss: 0.3456 - val_acc: 0.8525\n",
      "Saved model to disk\n",
      "17088/17088 [==============================] - 134s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "kwargs = dict(fl1=128, fl2= 16, fl3=16, kl=5, dl=32, optimizer= ''.join('Nadam'), layer=3)\n",
    "model = embeddings(**kwargs)\n",
    "model.fit(x_train, y_train, batch_size= 16, epochs=2, validation_data=(x_val, y_val))\n",
    "model_json = model.to_json()\n",
    "with open(\"model_embeddings_3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_embeddings_3.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "model3_test_y = model.predict(x_test, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4th Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 233s 15ms/step - loss: 0.5724 - acc: 0.6875 - val_loss: 0.3983 - val_acc: 0.7444\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 229s 14ms/step - loss: 0.4042 - acc: 0.7015 - val_loss: 0.4025 - val_acc: 0.7619\n",
      "Saved model to disk\n",
      "17088/17088 [==============================] - 98s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "kwargs = dict(fl1=64, fl2= 64, fl3=0, kl=5, dl=64, optimizer= ''.join('Nadam'), layer=2)\n",
    "model = embeddings(**kwargs)\n",
    "model.fit(x_train, y_train, batch_size= 16, epochs=2, validation_data=(x_val, y_val))\n",
    "model_json = model.to_json()\n",
    "with open(\"model_embeddings_4.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_embeddings_4.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "model4_test_y = model.predict(x_test, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5th Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 324s 20ms/step - loss: 0.5277 - acc: 0.7188 - val_loss: 0.4324 - val_acc: 0.7649\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 325s 20ms/step - loss: 0.4147 - acc: 0.7373 - val_loss: 0.3594 - val_acc: 0.8077\n",
      "Saved model to disk\n",
      "17088/17088 [==============================] - 139s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "kwargs = dict(fl1=128, fl2= 16, fl3=0, kl=5, dl=128, optimizer= ''.join('Nadam'), layer=2)\n",
    "model = embeddings(**kwargs)\n",
    "model.fit(x_train, y_train, batch_size= 16, epochs=2, validation_data=(x_val, y_val))\n",
    "model_json = model.to_json()\n",
    "with open(\"model_embeddings_5.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_embeddings_5.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "model5_test_y = model.predict(x_test, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(pred_test_y, actuals):\n",
    "\n",
    "    predictions =[]\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "\n",
    "    for i in range(len(pred_test_y)):\n",
    "        if (pred_test_y[i]>=0.5):\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    \n",
    "    for i in range (len(pred_test_y)):\n",
    "        if ((predictions[i]==1) & (actuals[i]==1)):\n",
    "            true_pos = true_pos+1\n",
    "        elif((predictions[i]==0) & (actuals[i]==0)):\n",
    "            true_neg = true_neg+1\n",
    "        elif((predictions[i]==1) & (actuals[i]==0)):\n",
    "            false_pos = false_pos +1\n",
    "        elif((predictions[i]==0) & (actuals[i]==1)):\n",
    "            false_neg = false_neg+1\n",
    "    prec=true_pos/(true_pos+false_pos)\n",
    "    recall = true_pos/(true_pos+false_neg)\n",
    "    accur=(true_pos+true_neg)/(true_pos+false_pos+ true_neg+ false_neg)\n",
    "    F1=2*(prec*recall/(prec+recall))\n",
    "    FPR = false_pos/(false_pos+true_neg)\n",
    "    return (true_pos, false_pos, true_neg, false_neg, prec,FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6603, 5894, 2169, 2422, 0.5283668080339281, 0.7309934267642316)\n",
      "(5133, 4572, 3491, 3892, 0.528902627511592, 0.567034602505271)\n",
      "(4899, 4377, 3686, 4126, 0.5281371280724451, 0.5428500558104924)\n",
      "(5539, 4947, 3116, 3486, 0.5282281136753767, 0.613543346149076)\n",
      "(5287, 4698, 3365, 3738, 0.529494241362043, 0.5826615403695895)\n"
     ]
    }
   ],
   "source": [
    "print((F1_score(model1_test_y, labels_test_set)))\n",
    "print((F1_score(model2_test_y, labels_test_set)))\n",
    "print((F1_score(model3_test_y, labels_test_set)))\n",
    "print((F1_score(model4_test_y, labels_test_set)))\n",
    "print((F1_score(model5_test_y, labels_test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
