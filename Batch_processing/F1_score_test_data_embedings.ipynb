{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/titli/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing function from a different ipny \n",
    "import pandas as pd\n",
    "import collections\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import operator\n",
    "from itertools import product\n",
    "import sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from functools import reduce\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "pd.options.display.max_columns = 1000\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Input, BatchNormalization, MaxPooling1D, Bidirectional,LSTM\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPool1D, Flatten , Embedding, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#https://www.amazon.com/Neural-Networks-Deep-Learning-Textbook/dp/3319944622/ref=cm_cr_arp_d_product_top?ie=UTF8\n",
    "#https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NUM_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>isspam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Confidential :Soma:, Ci@lis, :P:ntermin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¢Â ÇªÁüÇÑ ¼³³¯ ÀÌº¥Æ® ÀÀ¸ðÇØ¼­ ºÎ¸ð´Ô²² ¼±¹°ÇÏ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_na_</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>StOck 0ppurtunities - their sh0Oting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>All your prescr[iption needs right here</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Subject  isspam\n",
       "0            Confidential :Soma:, Ci@lis, :P:ntermin       1\n",
       "1  ¢Â ÇªÁüÇÑ ¼³³¯ ÀÌº¥Æ® ÀÀ¸ðÇØ¼­ ºÎ¸ð´Ô²² ¼±¹°ÇÏ...       1\n",
       "2                                               _na_       1\n",
       "3               StOck 0ppurtunities - their sh0Oting       1\n",
       "6            All your prescr[iption needs right here       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_data = pd.read_csv('subject_spam.csv', index_col=0, encoding='utf8', engine='python')\n",
    "keras_data.fillna(\"_na_\", inplace = True)\n",
    "keras_data = keras_data.drop(keras_data[keras_data.isspam == \"_na_\"].index)\n",
    "spammer = {'spam ': 1,'ham ': 0} \n",
    "keras_data.isspam = [spammer[item] for item in keras_data.isspam] \n",
    "spam_index = keras_data[keras_data.isspam ==1].index\n",
    "ham_index = keras_data[keras_data.isspam == 0].index\n",
    "new_index = np.concatenate((spam_index[:10000], ham_index[:9997]), axis=0)\n",
    "keras_data_new = keras_data.iloc[new_index]\n",
    "labels = keras_data_new.isspam\n",
    "keras_data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join('glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15425 unique tokens.\n",
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 2)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(keras_data_new.Subject)\n",
    "sequences = tokenizer.texts_to_sequences(keras_data_new.Subject)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print('Training model.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>isspam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19223</th>\n",
       "      <td>RE: Daily Summary of Risk Data</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19224</th>\n",
       "      <td>Softwares CDS all software under 15$ and 99$!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19226</th>\n",
       "      <td>Hotel Room Bargains at up to 70% off!  Save in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19227</th>\n",
       "      <td>RE: Greeley Gas Company</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19228</th>\n",
       "      <td>Any Software just in under 15-99$, Xp-adobe etc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Subject  isspam\n",
       "19223                     RE: Daily Summary of Risk Data       0\n",
       "19224      Softwares CDS all software under 15$ and 99$!       1\n",
       "19226  Hotel Room Bargains at up to 70% off!  Save in...       1\n",
       "19227                            RE: Greeley Gas Company       0\n",
       "19228    Any Software just in under 15-99$, Xp-adobe etc       1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_data = pd.read_csv('subject_spam.csv', index_col=0, encoding='utf8', engine='python')\n",
    "keras_data.fillna(\"_na_\", inplace = True)\n",
    "keras_data = keras_data.drop(keras_data[keras_data.isspam == \"_na_\"].index)\n",
    "spammer = {'spam ': 1,'ham ': 0} \n",
    "keras_data.isspam = [spammer[item] for item in keras_data.isspam] \n",
    "spam_index = keras_data[keras_data.isspam ==1].index\n",
    "ham_index = keras_data[keras_data.isspam == 0].index\n",
    "new_index = np.concatenate((spam_index[10000:19000], ham_index[9997:19000]), axis=0)\n",
    "keras_data_test_set = keras_data.iloc[new_index]\n",
    "labels_test_set = keras_data_test_set.isspam\n",
    "keras_data_test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15425 unique tokens.\n",
      "Shape of data tensor: (17088, 1000)\n",
      "Shape of label tensor: (19997, 2)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(keras_data_new.Subject)\n",
    "sequences = tokenizer.texts_to_sequences(keras_data_test_set.Subject)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#labels_test_set = to_categorical(np.asarray(labels_test_set))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "#labels_test_set = labels_test_set[indices]\n",
    "x_test = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = pd.read_csv('embeddings_models1.csv')\n",
    "embedding = pd.read_csv('embeddings_models.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dl</th>\n",
       "      <th>fl1</th>\n",
       "      <th>fl2</th>\n",
       "      <th>fl3</th>\n",
       "      <th>kl</th>\n",
       "      <th>layer</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>trainable_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.909676</td>\n",
       "      <td>0.206020</td>\n",
       "      <td>0.897224</td>\n",
       "      <td>0.242720</td>\n",
       "      <td>16594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.910114</td>\n",
       "      <td>0.212017</td>\n",
       "      <td>0.899725</td>\n",
       "      <td>0.243949</td>\n",
       "      <td>45986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.905988</td>\n",
       "      <td>0.218607</td>\n",
       "      <td>0.901975</td>\n",
       "      <td>0.245111</td>\n",
       "      <td>22338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.909301</td>\n",
       "      <td>0.206974</td>\n",
       "      <td>0.897974</td>\n",
       "      <td>0.245414</td>\n",
       "      <td>17154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>0.909551</td>\n",
       "      <td>0.212853</td>\n",
       "      <td>0.897974</td>\n",
       "      <td>0.248456</td>\n",
       "      <td>24866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dl  fl1  fl2  fl3  kl  layer optimizer  train_acc  train_loss   val_acc  \\\n",
       "297   16   32   32   64   5      1     Nadam   0.909676    0.206020  0.897224   \n",
       "359   32   32   32  128   5      3     Nadam   0.910114    0.212017  0.899725   \n",
       "47   128   32   16   16   5      3     Nadam   0.905988    0.218607  0.901975   \n",
       "399   32   32   64   16   5      1      Adam   0.909301    0.206974  0.897974   \n",
       "107   16   32   16   64   5      3     Nadam   0.909551    0.212853  0.897974   \n",
       "\n",
       "     val_loss  trainable_params  \n",
       "297  0.242720             16594  \n",
       "359  0.243949             45986  \n",
       "47   0.245111             22338  \n",
       "399  0.245414             17154  \n",
       "107  0.248456             24866  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat([embedding1, embedding], axis=0)\n",
    "result_sort = result.sort_values(['val_loss'])\n",
    "result_sort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(fl1=32, fl2=32, fl3=64, dl=16, optimizer= 'RMSprop', kl = 5, layer =1 ):\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    if (layer == 1):\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "    elif (layer == 2):\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl2, kernel_size = kl, activation='relu')(x)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        \n",
    "    else:\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl2, kernel_size = kl, activation='relu')(x)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl3, kernel_size = kl, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(units = dl, activation='relu')(x)\n",
    "    preds = Dense(2, activation='softmax')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss= 'categorical_crossentropy',\n",
    "              optimizer= optimizer,\n",
    "              metrics=['acc'])\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.3466 - acc: 0.8469 - val_loss: 0.2931 - val_acc: 0.8747\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 64s 4ms/step - loss: 0.2493 - acc: 0.8915 - val_loss: 0.3223 - val_acc: 0.8740\n",
      "17088/17088 [==============================] - 29s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "kwargs = dict(fl1=32, fl2= 0, fl3=0, kl=5, dl=64, optimizer= ''.join('Nadam'), layer=1)\n",
    "model = embeddings(**kwargs)\n",
    "model.fit(x_train, y_train, batch_size= 16, epochs=2, validation_data=(x_val, y_val))\n",
    "model1_test_y = model.predict(x_test, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 71s 4ms/step - loss: 0.3468 - acc: 0.8453 - val_loss: 0.3535 - val_acc: 0.8555\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 73s 5ms/step - loss: 0.2575 - acc: 0.8900 - val_loss: 0.2790 - val_acc: 0.8737\n",
      "17088/17088 [==============================] - 32s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "kwargs = dict(fl1=32, fl2= 32, fl3=128, kl=5,dl=32, optimizer= ''.join('Nadam'), layer=3)\n",
    "model = embeddings(**kwargs)\n",
    "model.fit(x_train, y_train, batch_size= 16, epochs=2, validation_data=(x_val, y_val))\n",
    "model2_test_y = model.predict(x_test, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 68s 4ms/step - loss: 0.3494 - acc: 0.8434 - val_loss: 0.3224 - val_acc: 0.8630\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 66s 4ms/step - loss: 0.2583 - acc: 0.8902 - val_loss: 0.2828 - val_acc: 0.8840\n",
      "17088/17088 [==============================] - 31s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "kwargs = dict(fl1=32, fl2= 16, fl3=16, kl=5, dl=128, optimizer= ''.join('Nadam'), layer=3)\n",
    "model = embeddings(**kwargs)\n",
    "model.fit(x_train, y_train, batch_size= 16, epochs=2, validation_data=(x_val, y_val))\n",
    "model3_test_y = model.predict(x_test, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_y = 0.33*model1_test_y + 0.33*model2_test_y + 0.34*model3_test_y\n",
    "A = pred_test_y[:,0]\n",
    "B = pred_test_y[:,1]\n",
    "pred_array =[]\n",
    "\n",
    "for i in range(len(pred_test_y)):\n",
    "    if (A[i]>0.99):\n",
    "        pred_array.append(0)\n",
    "    else:\n",
    "        pred_array.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score on test set is 66.00%\n"
     ]
    }
   ],
   "source": [
    "predictions = pred_array\n",
    "actuals = labels_test_set.values\n",
    "true_pos= 0\n",
    "true_neg = 0\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "for i in range (17088):\n",
    "    if ((predictions[i]==1) & (actuals[i]==1)):\n",
    "        true_pos = true_pos+1\n",
    "    elif((predictions[i]==0) & (actuals[i]==0)):\n",
    "        true_neg = true_neg+1\n",
    "    elif((predictions[i]==1) & (actuals[i]==0)):\n",
    "        false_pos = false_pos +1\n",
    "    elif((predictions[i]==0) & (actuals[i]==1)):\n",
    "        false_neg = false_neg+1\n",
    "prec=true_pos/(true_pos+false_pos)\n",
    "recall = true_pos/(true_pos+false_neg)\n",
    "accur=(true_pos+true_neg)/(true_pos+false_pos+ true_neg+ false_neg)\n",
    "F1=2*(prec*recall/(prec+recall))\n",
    "print('F1-score on test set is {0:.2f}%'.format(round(F1*100),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
