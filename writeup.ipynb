{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import plotly.plotly \n",
    "from plotly.offline import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"spam-email.jpg\">\n",
    "<br>\n",
    "This study is about designing a spam filter that can separate ham and spam emails based on various machine learning techniques. The subject line of the emails are analyzed based on the NLP technique of tokenizing sentences. Document/Text classification is one of the important and typical task in supervised machine learning (ML). Assigning categories to documents, such as email in this study has many applications like e.g. spam filtering. In this article, I would like to demonstrate and compare three architectures of neural network to classifying emails and create a spam filter. Detailed hyperparameter analysis are performed on these three archictectures. **Validation loss** is taken as the metrics to caolculate the best model. Models are sorted based on the lowest validation loss metrics. \n",
    "<br>\n",
    "The dataset is based on cleaned Enron corpus, there are a total of 92188 messages belonging to 158 users with an average of 757 messages per user. The dataset has almost an equal distribution of ham and spam emails. 19997 emails consisting of ham and spams are used for train and validation set. A validation split of 20% is used. After performing a series of Hyperparameter analysis three best models for each of the architecture are choosen for analysis on test dataset that comprises of 17880 emails.\n",
    "<img src = 'sshot.png'>\n",
    "### Neural network for spam detection\n",
    "<img src= \"Capture.PNG\">\n",
    "\n",
    "Neural networks are powerful machine learning algorithms. They can be used to transform the features so as to form fairly complex non linear decision boundaries. They are primarily used for classification problems. The fully connected layer takes the deep representation from the RNN/LSTM and transforms it into the final output classes or class scores. This component is comprised of fully connected layers along with batch normalization and optionally dropout layers for regularization.\n",
    "### Word Embedding\n",
    "A word is a basic unit of language that conveys meaning of its own. With the help of words and language rules, an infinite set of concepts can be expressed. Machine learning approaches towards NLP require words to be expressed in vector form. Word embeddings, proposed in 1986 [4], is a feature engineering technique in which words are represented as a vector.\n",
    "Word embedding is a technique for representing the meaning of a word in terms other words as defined by the Word2vec approach. Embeddings are designed for specific tasks. Let's take a simple way to represent a word in vector space: each word is uniquely mapped onto a series of zeros and a one, with the location of the one corresponding to the index of the word in the vocabulary. This technique is referred to as one-hot encoding. As an example, take the sentence \"Go Intelligent Bot Service Artificial Intelligence, Oracle\":   \n",
    "<img src= \"1.png\">\n",
    "The embedding of the word vectors enables the identification of words that are used in similar contexts to a specific word. In word embedding the words that have the same meaning have a similar representation.\n",
    "<br>\n",
    "In this study Glove (Global vectors)is used which is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. \n",
    "GloVe is essentially a log-bilinear model with a weighted least-squares objective. The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning. For example, consider the co-occurrence probabilities for target words ice and steam with various probe words from the vocabulary. Here are some actual probabilities from a 6 billion word corpus: \n",
    "<img src= \"table.png\">\n",
    "<br>\n",
    "The training objective of GloVe is to learn word vectors such that their dot product equals the logarithm of the words' probability of co-occurrence. Owing to the fact that the logarithm of a ratio equals the difference of logarithms, this objective associates (the logarithm of) ratios of co-occurrence probabilities with vector differences in the word vector space. Because these ratios can encode some form of meaning, this information gets encoded as vector differences as well. For this reason, the resulting word vectors perform very well on word analogy tasks, such as those examined in the word2vec package. \n",
    "$$J = \\sum_{i,j=1}^{V} f (X_{ij})(w^T\\tilde{w_j} + b_i + \\tilde{b_j} - logX_{ij})^2$$\n",
    "where V is the size of the vocabulary. Let the matrix of word-word co-occurrence counts be denoted by $X$, whose entries $X_{ij}$ tabulate the number of times word $j$ occurs in the context of word $i$.\n",
    "### Vizualizing hyperparameters\n",
    "#### CNN\n",
    "<img src= \"CNN.png\">\n",
    "#### LSTM\n",
    "<img src= \"LSTM.png\">\n",
    "\n",
    "<img src= \"CNN3d.png\">\n",
    "<img src= \"LSTM3d.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(fl1=32, fl2=32, fl3=64, dl=16, optimizer= 'RMSprop', kl = 5, layer =1 ):\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    if (layer == 1):\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "    elif (layer == 2):\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl2, kernel_size = kl, activation='relu')(x)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        \n",
    "    else:\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(embedded_sequences)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl2, kernel_size = kl, activation='relu')(x)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl3, kernel_size = kl, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(units = dl, activation='relu')(x)\n",
    "    preds = Dense(2, activation='tanh')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss= 'categorical_crossentropy',\n",
    "              optimizer= optimizer,\n",
    "              metrics=['acc'])\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_LSTM(fl1=16, fl2=16, fl3=16, dl=16, optimizer= 'RMSprop', kl = 5, layer =1): \n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Bidirectional(LSTM(units = fl1, return_sequences=True))(embedded_sequences)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(units=dl, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss= 'categorical_crossentropy',\n",
    "              optimizer= optimizer,\n",
    "              metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_embeddings(fl1=32, fl2=32, fl3=64, dl=16, optimizer= 'Nadam', kl = 5, layer =1 ):\n",
    "    inp =  Input(shape=(1000, 1))\n",
    "    if (layer == 1):\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(inp)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "    elif (layer == 2):\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(inp)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl2, kernel_size = kl, activation='relu')(x)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "    else:\n",
    "        x = Conv1D(filters = fl1, kernel_size = kl, activation='relu')(inp)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl2, kernel_size = kl, activation='relu')(x)\n",
    "        x = MaxPooling1D(pool_size = kl)(x)\n",
    "        x = Conv1D(filters = fl3, kernel_size = kl, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(units = dl, activation='relu')(x)\n",
    "    preds = Dense(1, activation='tanh')(x)\n",
    "    model = Model(inp, preds)\n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "              optimizer= optimizer,\n",
    "              metrics=['acc'])\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neurons Dense layer</th>\n",
       "      <th>Filter 1st layer</th>\n",
       "      <th>Filter 2nd layer</th>\n",
       "      <th>Filter 3rd layer</th>\n",
       "      <th>kernel</th>\n",
       "      <th>layer</th>\n",
       "      <th>trainable_params</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>ANN</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>66226</td>\n",
       "      <td>Adam</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4688</td>\n",
       "      <td>3904</td>\n",
       "      <td>4159</td>\n",
       "      <td>4337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>75602</td>\n",
       "      <td>Adam</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4538</td>\n",
       "      <td>4032</td>\n",
       "      <td>4031</td>\n",
       "      <td>4487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>76290</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.23</td>\n",
       "      <td>6152</td>\n",
       "      <td>2621</td>\n",
       "      <td>5442</td>\n",
       "      <td>2873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>56898</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4423</td>\n",
       "      <td>4192</td>\n",
       "      <td>3871</td>\n",
       "      <td>4602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>76818</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4294</td>\n",
       "      <td>4323</td>\n",
       "      <td>3740</td>\n",
       "      <td>4731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>251074</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.18</td>\n",
       "      <td>9025</td>\n",
       "      <td>0</td>\n",
       "      <td>8063</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38338</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4940</td>\n",
       "      <td>3722</td>\n",
       "      <td>4341</td>\n",
       "      <td>4085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>242786</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4876</td>\n",
       "      <td>3780</td>\n",
       "      <td>4283</td>\n",
       "      <td>4149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>92866</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4978</td>\n",
       "      <td>3599</td>\n",
       "      <td>4464</td>\n",
       "      <td>4047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42626</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4413</td>\n",
       "      <td>4159</td>\n",
       "      <td>3904</td>\n",
       "      <td>4612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5106</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>no_embeddings</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.66</td>\n",
       "      <td>72</td>\n",
       "      <td>7978</td>\n",
       "      <td>85</td>\n",
       "      <td>8953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>26898</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>no_embeddings</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "      <td>8063</td>\n",
       "      <td>0</td>\n",
       "      <td>9025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>30114</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>no_embeddings</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "      <td>8063</td>\n",
       "      <td>0</td>\n",
       "      <td>9025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4546</td>\n",
       "      <td>Adam</td>\n",
       "      <td>no_embeddings</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0</td>\n",
       "      <td>8063</td>\n",
       "      <td>0</td>\n",
       "      <td>9025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>16114</td>\n",
       "      <td>Adam</td>\n",
       "      <td>no_embeddings</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.67</td>\n",
       "      <td>18</td>\n",
       "      <td>8039</td>\n",
       "      <td>24</td>\n",
       "      <td>9007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Neurons Dense layer  Filter 1st layer  Filter 2nd layer  Filter 3rd layer  \\\n",
       "0                    16               128                16                32   \n",
       "1                    64               128                16                64   \n",
       "2                    32               128                16                16   \n",
       "3                    64                64                64                32   \n",
       "4                   128               128                16                16   \n",
       "5                    64               128                 0                 0   \n",
       "6                    64                32                 0                 0   \n",
       "7                    32               128                 0                 0   \n",
       "8                    64                64                 0                 0   \n",
       "9                   128                32                 0                 0   \n",
       "10                   32                16                16                32   \n",
       "11                   16                16                 4                 4   \n",
       "12                   64                16                64                64   \n",
       "13                   16                16                16                32   \n",
       "14                   16                16                64                32   \n",
       "\n",
       "    kernel  layer  trainable_params optimizer            ANN  train_acc  \\\n",
       "0        5      1             66226      Adam            CNN       0.92   \n",
       "1        5      2             75602      Adam            CNN       0.92   \n",
       "2        5      3             76290     Nadam            CNN       0.92   \n",
       "3        5      2             56898     Nadam            CNN       0.92   \n",
       "4        5      2             76818     Nadam            CNN       0.92   \n",
       "5        0      1            251074     Nadam           LSTM       0.93   \n",
       "6        0      1             38338     Nadam           LSTM       0.93   \n",
       "7        0      1            242786     Nadam           LSTM       0.93   \n",
       "8        0      1             92866     Nadam           LSTM       0.93   \n",
       "9        0      1             42626     Nadam           LSTM       0.93   \n",
       "10       5      3              5106     Nadam  no_embeddings       0.57   \n",
       "11       5      3             26898     Nadam  no_embeddings       0.56   \n",
       "12       5      3             30114     Nadam  no_embeddings       0.56   \n",
       "13       5      3              4546      Adam  no_embeddings       0.57   \n",
       "14       5      3             16114      Adam  no_embeddings       0.57   \n",
       "\n",
       "    train_loss  val_acc  val_loss    TP    TN    FP    FN  \n",
       "0         0.18     0.90      0.23  4688  3904  4159  4337  \n",
       "1         0.19     0.90      0.23  4538  4032  4031  4487  \n",
       "2         0.19     0.90      0.23  6152  2621  5442  2873  \n",
       "3         0.20     0.90      0.23  4423  4192  3871  4602  \n",
       "4         0.19     0.90      0.23  4294  4323  3740  4731  \n",
       "5         0.16     0.93      0.18  9025     0  8063     0  \n",
       "6         0.18     0.92      0.18  4940  3722  4341  4085  \n",
       "7         0.18     0.92      0.19  4876  3780  4283  4149  \n",
       "8         0.16     0.93      0.19  4978  3599  4464  4047  \n",
       "9         0.18     0.93      0.19  4413  4159  3904  4612  \n",
       "10        0.67     0.58      0.66    72  7978    85  8953  \n",
       "11        0.67     0.57      0.66     0  8063     0  9025  \n",
       "12        0.69     0.57      0.66     0  8063     0  9025  \n",
       "13        0.67     0.56      0.67     0  8063     0  9025  \n",
       "14        0.68     0.59      0.67    18  8039    24  9007  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('test_loss.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam emails: 9025\n",
      "Ham emails: 8063\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neurons Dense layer</th>\n",
       "      <th>Filter 1st layer</th>\n",
       "      <th>Filter 2nd layer</th>\n",
       "      <th>Filter 3rd layer</th>\n",
       "      <th>kernel</th>\n",
       "      <th>layer</th>\n",
       "      <th>trainable_params</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>ANN</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>251074</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.18</td>\n",
       "      <td>9025</td>\n",
       "      <td>0</td>\n",
       "      <td>8063</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38338</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4940</td>\n",
       "      <td>3722</td>\n",
       "      <td>4341</td>\n",
       "      <td>4085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>242786</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4876</td>\n",
       "      <td>3780</td>\n",
       "      <td>4283</td>\n",
       "      <td>4149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>92866</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4978</td>\n",
       "      <td>3599</td>\n",
       "      <td>4464</td>\n",
       "      <td>4047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42626</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4413</td>\n",
       "      <td>4159</td>\n",
       "      <td>3904</td>\n",
       "      <td>4612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>66226</td>\n",
       "      <td>Adam</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4688</td>\n",
       "      <td>3904</td>\n",
       "      <td>4159</td>\n",
       "      <td>4337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>75602</td>\n",
       "      <td>Adam</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4538</td>\n",
       "      <td>4032</td>\n",
       "      <td>4031</td>\n",
       "      <td>4487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>76290</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.23</td>\n",
       "      <td>6152</td>\n",
       "      <td>2621</td>\n",
       "      <td>5442</td>\n",
       "      <td>2873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>56898</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4423</td>\n",
       "      <td>4192</td>\n",
       "      <td>3871</td>\n",
       "      <td>4602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>76818</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>CNN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4294</td>\n",
       "      <td>4323</td>\n",
       "      <td>3740</td>\n",
       "      <td>4731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5106</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>no_embeddings</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.66</td>\n",
       "      <td>72</td>\n",
       "      <td>7978</td>\n",
       "      <td>85</td>\n",
       "      <td>8953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>26898</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>no_embeddings</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "      <td>8063</td>\n",
       "      <td>0</td>\n",
       "      <td>9025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>30114</td>\n",
       "      <td>Nadam</td>\n",
       "      <td>no_embeddings</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "      <td>8063</td>\n",
       "      <td>0</td>\n",
       "      <td>9025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4546</td>\n",
       "      <td>Adam</td>\n",
       "      <td>no_embeddings</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0</td>\n",
       "      <td>8063</td>\n",
       "      <td>0</td>\n",
       "      <td>9025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>16114</td>\n",
       "      <td>Adam</td>\n",
       "      <td>no_embeddings</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.67</td>\n",
       "      <td>18</td>\n",
       "      <td>8039</td>\n",
       "      <td>24</td>\n",
       "      <td>9007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Neurons Dense layer  Filter 1st layer  Filter 2nd layer  Filter 3rd layer  \\\n",
       "5                    64               128                 0                 0   \n",
       "6                    64                32                 0                 0   \n",
       "7                    32               128                 0                 0   \n",
       "8                    64                64                 0                 0   \n",
       "9                   128                32                 0                 0   \n",
       "0                    16               128                16                32   \n",
       "1                    64               128                16                64   \n",
       "2                    32               128                16                16   \n",
       "3                    64                64                64                32   \n",
       "4                   128               128                16                16   \n",
       "10                   32                16                16                32   \n",
       "11                   16                16                 4                 4   \n",
       "12                   64                16                64                64   \n",
       "13                   16                16                16                32   \n",
       "14                   16                16                64                32   \n",
       "\n",
       "    kernel  layer  trainable_params optimizer            ANN  train_acc  \\\n",
       "5        0      1            251074     Nadam           LSTM       0.93   \n",
       "6        0      1             38338     Nadam           LSTM       0.93   \n",
       "7        0      1            242786     Nadam           LSTM       0.93   \n",
       "8        0      1             92866     Nadam           LSTM       0.93   \n",
       "9        0      1             42626     Nadam           LSTM       0.93   \n",
       "0        5      1             66226      Adam            CNN       0.92   \n",
       "1        5      2             75602      Adam            CNN       0.92   \n",
       "2        5      3             76290     Nadam            CNN       0.92   \n",
       "3        5      2             56898     Nadam            CNN       0.92   \n",
       "4        5      2             76818     Nadam            CNN       0.92   \n",
       "10       5      3              5106     Nadam  no_embeddings       0.57   \n",
       "11       5      3             26898     Nadam  no_embeddings       0.56   \n",
       "12       5      3             30114     Nadam  no_embeddings       0.56   \n",
       "13       5      3              4546      Adam  no_embeddings       0.57   \n",
       "14       5      3             16114      Adam  no_embeddings       0.57   \n",
       "\n",
       "    train_loss  val_acc  val_loss    TP    TN    FP    FN  \n",
       "5         0.16     0.93      0.18  9025     0  8063     0  \n",
       "6         0.18     0.92      0.18  4940  3722  4341  4085  \n",
       "7         0.18     0.92      0.19  4876  3780  4283  4149  \n",
       "8         0.16     0.93      0.19  4978  3599  4464  4047  \n",
       "9         0.18     0.93      0.19  4413  4159  3904  4612  \n",
       "0         0.18     0.90      0.23  4688  3904  4159  4337  \n",
       "1         0.19     0.90      0.23  4538  4032  4031  4487  \n",
       "2         0.19     0.90      0.23  6152  2621  5442  2873  \n",
       "3         0.20     0.90      0.23  4423  4192  3871  4602  \n",
       "4         0.19     0.90      0.23  4294  4323  3740  4731  \n",
       "10        0.67     0.58      0.66    72  7978    85  8953  \n",
       "11        0.67     0.57      0.66     0  8063     0  9025  \n",
       "12        0.69     0.57      0.66     0  8063     0  9025  \n",
       "13        0.67     0.56      0.67     0  8063     0  9025  \n",
       "14        0.68     0.59      0.67    18  8039    24  9007  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Spam emails: 9025')\n",
    "print('Ham emails: 8063')\n",
    "df_sort = df.sort_values('val_loss')\n",
    "df_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
